<!DOCTYPE html>
<html lang="en">
  <html>
    <head>
      <meta charset="UTF-8" />
      <title>Music 220A Final Project | Sebastian James | Spring '21</title>
      <link rel="stylesheet" type="text/css" href="./css/style.css" />
      <!-- Add icon library -->
      <link rel="stylesheet" 
      href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    </head>
    <body>
      <a href="https://ccrma.stanford.edu/~sebaxj/" class="button1"
        >Return to Homepage</a
      ><br />

      <h2 id="main-header-220a-sebastian">Music 220A: Final Project | Spring 2021</h2>
      <h4 id="author-sebastian-james">Sebastian x. James</h4>

      <h1></h1>
      <h3 id="final-name">An Audiovisual Exhibit with Processing and ChucK</h3>
      <p>
          This is an exhibit featuring an audiovisual pipeline built using Processing
          and ChucK. A <a href="https://processing.org">Processing</a> script accepts a 
          movie input and uses frame differentiation â€“ a technique for video processing where 
          the pixels of the current frame are compared to the previous in a bit mask. 
          Assuming that the number of pixels remaining from the bit mask, dubbed the sum of movement 
          for the frame, is greater than a threshold to filter out noise, a value of percent change 
          is sent to a ChucK receiver over open sound control (OSC). Percent change between frames is 
          correlated to sound density generated by a Synth class in the ChucK script.
      </p>
      <p>
          Though this portfolio exhibits pre-recorded movie, the project could be adapted to 
          live video input. Applications for this project span artistic endeavors from human-computer 
          interaction sound synthesis, to performance, to audiovisual experiences.
      </p>

      <video alt="original video used for exhibition" width="680" controls preload>
        <source src="app/data/test.mov" type="video/mp4">
        Your browser does not support HTML video.
      </video>

      <video alt="output using frame differentiation on video input and sound generated by motion" 
        width="680" controls preload>
        <source src="assets/final-out.mp4" type="video/mp4">
        Your browser does not support HTML video.
      </video>

      <h3 id="reflection">A Reflection on this Project</h3>
      <p>
          The first hurdle to overcome was finding an efficient way to process video and trigger audio with 
          the lowest latency possible. Though an algorithm like Background Subtraction run through the OpenCV 
          library would most accurately determine motion of a foreground object against a background, the process 
          is incredibly CPU intensive and resulted in a high latency and low fidelity of a combined audio-visual 
          output. Frame differentiation, on the other hand, is a more streamlined approach dealing at the extreme 
          of literally comparing each RGB component of each pixel to determine is movement occurred. This process 
          was enhanced using bitwise operators and bit-masking. Ultimately, I chose to harness the latency as "artful 
          design" and added multiple reverb filters and delay lines to the ChucK audio. 
      </p>
      <p>
          However, as one can imagine, such extreme pixel differentiation resulted in noise convoluting what was intentional 
          motion in the input clip. As such, certain pixel movement thresholds were needed to filter out noise, auto-focus, 
          flickering lights, and other artifacts. A value of percent change between the current frame and the one previous is sent 
          every 10 million frames over OSC to the ChucK receiver script. Messages are sent every 10 million frames because anything more 
          resulted in high latency between video and audio, and anything less resulted in pixel movement being lost. This system of 
          movement thresholds and OSC message rates was calibrated using a "control" video of a room with no movement.
      </p>
      <p>
          Lastly, the ChucK script generating audio upon movement triggers over OSC uses a Synth codebase I wrote, the STK flute code 
          which is adapted, and a kick drum sample played through a SndBuf. These three classes work in trichotomy to respond to certain 
          levels of change and play audio accordingly. Reverb, delay, and random number generators work to create an ever-evolving soundscape 
          in this audiovisual experience.  
      </p>

      <h1></h1>

      <h3>Archive</h3>
      <a href="pages/milestone1.html" class="button1">Milestone 1</a> <br />
      <a href="pages/milestone2.html" class="button1">Milestone 2</a> <br />

      <h1></h1>

      <center><h3>Repository</h3></center>
      <center><a href="app/app.pde" class="button1">Processing app code</a>
        <a href="app/OSC_recv.ck" class="button1">ChucK receiver code</a><br /></center>
      <center><p>View on GitHub</p></center>
      <center><p><a href="https://github.com/sebaxj/music220a" class="fa fa-github fa_custom fa-4x"></a></p></center>
    </body>
  </html>
</html>
